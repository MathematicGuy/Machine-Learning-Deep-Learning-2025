{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2ec7fc2",
      "metadata": {
        "id": "a2ec7fc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b75fb65d-57a0-4ed7-cf13-5a17ff063180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=141MgG4CC7XffVH32hQy7lQ0PKCafskji\n",
            "From (redirected): https://drive.google.com/uc?id=141MgG4CC7XffVH32hQy7lQ0PKCafskji&confirm=t&uuid=7674a63a-0070-49a0-ae4b-12c1ae9db0ea\n",
            "To: /content/HMDB51.zip\n",
            " 16% 528M/3.37G [00:05<00:20, 139MB/s]"
          ]
        }
      ],
      "source": [
        "# !pip install -qq torch timm gdown tqdm\n",
        "\n",
        "!gdown 141MgG4CC7XffVH32hQy7lQ0PKCafskji\n",
        "!mkdir -p hmdb51_data\n",
        "!unzip -q HMDB51.zip -d hmdb51_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34938f4f",
      "metadata": {
        "id": "34938f4f"
      },
      "source": [
        "# LS-ViT HMDB51 Training Notebook\n",
        "Notebook này tải dữ liệu HMDB51 frame, định nghĩa lại toàn bộ model và pipeline training từ đầu, kèm visualization cho data, metrics và inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccd0b6ae",
      "metadata": {
        "id": "ccd0b6ae"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import amp\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import timm\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {DEVICE}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```js\n",
        "================================================================================\n",
        "              MINH HỌA LUỒNG DỮ LIỆU (WORKFLOW) - MODULE ATTENTION\n",
        "================================================================================\n",
        "\n",
        "KÝ HIỆU:\n",
        "  B : Batch size (Số lượng video/ảnh trong 1 lần train)\n",
        "  N : Số lượng Token (Số patches + cls_token)\n",
        "  C : Tổng số chiều Embedding (Ví dụ: 768)\n",
        "  H : Số lượng Heads (Ví dụ: 12)\n",
        "  D : Số chiều của mỗi Head (D = C // H, Ví dụ: 64)\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 1: ĐẦU VÀO & KHỞI TẠO QKV\n",
        "Code: qkv = self.qkv(x)\n",
        "--------------------------------------------------------------------------------\n",
        "[ INPUT ]\n",
        "    |\n",
        "    |  Tensor x:  [ B, N, C ]\n",
        "    |\n",
        "    v\n",
        "[ Lớp Linear (self.qkv) ] --> Nhân ma trận, mở rộng chiều gấp 3 lần\n",
        "    |\n",
        "    |  Tensor qkv: [ B, N, 3*C ]\n",
        "    |              (Chứa gộp cả Query, Key, Value nối liền nhau)\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 2: TÁCH & SẮP XẾP LẠI (RESHAPE & PERMUTE)\n",
        "Code: ...reshape(B, N, 3, H, D).permute(2, 0, 3, 1, 4)\n",
        "--------------------------------------------------------------------------------\n",
        "[ Reshape ] --> Tách chiều C thành (3, H, D)\n",
        "    |\n",
        "    |  Trung gian: [ B, N, 3, H, D ]\n",
        "    |\n",
        "    v\n",
        "[ Permute ] --> Đảo thứ tự trục để tối ưu tính toán song song\n",
        "    |           Đưa trục (3) lên đầu, gom các Heads lại gần nhau\n",
        "    |\n",
        "    |  Output:     [ 3, B, H, N, D ]\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 3: TÁCH Q, K, V\n",
        "Code: q, k, v = qkv.unbind(0)\n",
        "--------------------------------------------------------------------------------\n",
        "[ Unbind dim=0 ] --> Tách trục đầu tiên (số 3) thành 3 tensor riêng biệt\n",
        "    |\n",
        "    |---> Tensor Q (Query): [ B, H, N, D ]\n",
        "    |---> Tensor K (Key):   [ B, H, N, D ]\n",
        "    |---> Tensor V (Value): [ B, H, N, D ]\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 4: TÍNH ATTENTION SCORE (Ma trận tương đồng)\n",
        "Code: attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "--------------------------------------------------------------------------------\n",
        "[ Transpose K ] --> Đảo 2 chiều cuối của K để nhân ma trận\n",
        "    |  K.T shape: [ B, H, D, N ]\n",
        "    |\n",
        "[ Dot Product ( @ ) ] --> Q nhân với K.T\n",
        "    |  Phép tính: (N, D) x (D, N) -> (N, N)\n",
        "    |\n",
        "    |  Raw Score: [ B, H, N, N ] --> Ma trận vuông N*N\n",
        "    |\n",
        "[ Scale ] --> Nhân với hằng số (1/sqrt(D))\n",
        "    |  Output:    [ B, H, N, N ]\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 5: SOFTMAX & DROPOUT\n",
        "Code: attn = attn.softmax(dim=-1) ...\n",
        "--------------------------------------------------------------------------------\n",
        "[ Softmax ] --> Chuyển điểm số thành xác suất (tổng hàng = 1)\n",
        "    |\n",
        "    |  Attn Map:  [ B, H, N, N ] --> Đây là bản đồ sự chú ý\n",
        "    |\n",
        "[ Dropout ] --> Tắt ngẫu nhiên một số trọng số (để regularization)\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 6: TỔNG HỢP THÔNG TIN (WEIGHTED SUM)\n",
        "Code: x = attn @ v\n",
        "--------------------------------------------------------------------------------\n",
        "[ Dot Product ( @ ) ] --> Nhân bản đồ chú ý với nội dung V\n",
        "    |  Phép tính: (Attn Map) x (Value)\n",
        "    |             (N, N)     x (N, D)  -> (N, D)\n",
        "    |\n",
        "    |  Output:    [ B, H, N, D ] --> Mỗi token đã được cập nhật thông tin\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 7: GỘP HEADS & TRẢ VỀ KÍCH THƯỚC CŨ\n",
        "Code: x = x.transpose(1, 2).reshape(B, N, C)\n",
        "--------------------------------------------------------------------------------\n",
        "[ Transpose ] --> Đưa trục N về lại vị trí thứ 2\n",
        "    |  Shape:     [ B, N, H, D ]\n",
        "    |\n",
        "[ Reshape/Flatten ] --> Gộp (H, D) thành C (vì C = H * D)\n",
        "    |\n",
        "    |  Output:    [ B, N, C ] --> Về lại kích thước embedding ban đầu\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 8: PROJECTION CUỐI CÙNG (MIXING)\n",
        "Code: x = self.proj(x)\n",
        "--------------------------------------------------------------------------------\n",
        "[ Linear Proj ] --> Trộn thông tin giữa các Heads (vì nãy giờ chạy song song)\n",
        "    |\n",
        "    |  OUTPUT CUỐI CÙNG: [ B, N, C ]\n",
        "    v\n",
        "================================================================================\n",
        "```"
      ],
      "metadata": {
        "id": "CA7mm9Dr0436"
      },
      "id": "CA7mm9Dr0436"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b96a6ad7",
      "metadata": {
        "id": "b96a6ad7"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ViTConfig:\n",
        "    image_size: int = 224\n",
        "    patch_size: int = 16\n",
        "    in_chans: int = 3\n",
        "    embed_dim: int = 768\n",
        "    depth: int = 12\n",
        "    num_heads: int = 12\n",
        "    mlp_ratio: float = 4.0\n",
        "    drop_rate: float = 0.1\n",
        "    attn_drop_rate: float = 0.1\n",
        "    drop_path_rate: float = 0.1\n",
        "    qkv_bias: bool = True\n",
        "\n",
        "\n",
        "def ensure_dir(path: Optional[str]) -> Optional[str]:\n",
        "    if path is None:\n",
        "        return None\n",
        "    path_obj = Path(path)\n",
        "    path_obj.mkdir(parents=True, exist_ok=True)\n",
        "    return str(path_obj)\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.drop_prob == 0.0 or not self.training:\n",
        "            return x\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "        random_tensor.floor_()\n",
        "        return x.div(keep_prob) * random_tensor\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, config: ViTConfig):\n",
        "        super().__init__()\n",
        "        self.image_size = config.image_size\n",
        "        self.patch_size = config.patch_size\n",
        "        # đảm bảo patch_size khi slide khớp vs image_size\n",
        "        self.num_patches = (config.image_size // config.patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(\n",
        "            config.in_chans,\n",
        "            config.embed_dim,\n",
        "            kernel_size=config.patch_size,\n",
        "            stride=config.patch_size,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: [Batch_Number, Channel, H, W]\n",
        "        x = self.proj(x) # [B, Embed_Dim, H/P, W/P], P is the patches_size\n",
        "        # làm phẳng 2 chiều ko gian h/p * w/p thành 1 chiều N (số patches)\n",
        "        x = x.flatten(2)\n",
        "        # đổi chỗ chiều Embed_Dim và N để có dạng [B, N, Embed_Dim]\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, dim: int, mlp_ratio: float, drop: float):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(dim * mlp_ratio)\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int, qkv_bias: bool, attn_drop: float, proj_drop: float):\n",
        "        \"\"\"\n",
        "        Đầu vào: (B, N, C) (Batch, Số token / pixel ảnh dc tokenize, Số chiều embedding)\n",
        "        Đầu ra: (B, N, C) (Cùng kích thước đầu vào)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        # kích thước vector đặc trưng của mỗi \"head\"\n",
        "        head_dim = dim // num_heads\n",
        "\n",
        "        # scale factor (căn bậc 2 của d_k trong cthuc Attention) - ổn định gradient khi tính softmax\n",
        "        self.scale = head_dim ** -0.5 # 1/(head_dim^2)\n",
        "\n",
        "        # q - like google search query, k - value key in dict, id_card for value. v - token information.\n",
        "        # Example: q = sáu, k = [0,1,0,0], v = 6\n",
        "        #\n",
        "        # Thay vì khai báo 3 lớp Linear riêng biệt (3 * nn.Linear(dim, dim)),\n",
        "        # ta gộp chung vào 1 lớp Linear lớn với kích thước output gấp 3 lần.\n",
        "        # -> Giúp tính toán song song nhanh hơn trên GPU (Matrix Multiplication tối ưu hơn). Dùng slide của TA\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "\n",
        "        # trộn thông tin các Heads lại vs nhau\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # tính qkv. (B,N,C) - (B, N, 3*C)\n",
        "        # Reshape: Tách 3*C thành 3 phần, chia C thành num_heads * head_dim\n",
        "        # -> (B, N, 3, num_heads, head_dim)\n",
        "        # Permute: Đảo thứ tự để đưa kích thước (3, B, num_heads) lên đầu phục vụ tính toán song song\n",
        "        # -> (3, B, num_heads, N, head_dim)\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        # tách qkv -> 3 tensor q, k, v riêng biệt. Mỗi tensor có shape (B, num_heads, head_dim)\n",
        "        q, k, v = qkv.unbind(0)\n",
        "\n",
        "        # cthuc Attention Score Attention(Q, K) = softmax( (Q @ K.T) / sqrt(q_k) )\n",
        "        # K: (B, H, N, D) -> K.T(-2, -1) tranpose at index D and N: (B, H, D, N)\n",
        "        attn = (q @ k.tranpose((-2, -1))) * self.scale # scale = 1/(head_dim^2)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = attn @ v # Apply Attention cho từng Pixel value trong ảnh.\n",
        "        x = x.transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```js\n",
        "================================================================================\n",
        "              MINH HỌA WORKFLOW - SMIF MODULE (Short-term Motion)\n",
        "================================================================================\n",
        "\n",
        "KÝ HIỆU:\n",
        "  B, T: Batch, Time\n",
        "  C: Channels\n",
        "  H, W: Height, Width\n",
        "  Offset: Khoảng cách thời gian so với hiện tại\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 1: ĐẦU VÀO & CHUẨN BỊ\n",
        "Code: motion_accum = zeros_like(video)\n",
        "--------------------------------------------------------------------------------\n",
        "[ INPUT ] Tensor Video: [ B, T, C, H, W ]\n",
        "    |\n",
        "    v\n",
        "[ Khởi tạo Accumulator ] --> Tensor rỗng để cộng dồn chuyển động\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 2: VÒNG LẶP THỜI GIAN (CALCULATE DIFFERENCE)\n",
        "Code: for offset in range(...)\n",
        "--------------------------------------------------------------------------------\n",
        "Ví dụ tại thời điểm t:\n",
        "    |\n",
        "    +---> [ Lấy Frame t-k ] (Past)\n",
        "    |     Dùng torch.roll + Fix biên (nếu t=0 thì Past=Present)\n",
        "    |\n",
        "    +---> [ Lấy Frame t+k ] (Future)\n",
        "    |     Dùng torch.roll + Fix biên\n",
        "    |\n",
        "    +---> [ Tính toán sự khác biệt ]\n",
        "          Diff_Future = |Frame(t+k) - Frame(t)|\n",
        "          Diff_Past   = |Frame(t)   - Frame(t-k)|\n",
        "    |\n",
        "    v\n",
        "[ Cộng dồn vào Accumulator ]\n",
        "    Motion_Accum += (Diff_Future + Diff_Past)\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 3: LỌC NHIỄU (THRESHOLDING)\n",
        "Code: mask = (motion_map > threshold)\n",
        "--------------------------------------------------------------------------------\n",
        "[ Normalize ] --> Chia cho số lượng frame tham chiếu\n",
        "    |\n",
        "    v\n",
        "[ Threshold Gate ] --> Nếu chuyển động < 0.05 (rung nhẹ/nhiễu) --> Gán = 0\n",
        "    |                  Nếu chuyển động > 0.05 (vật thể di chuyển) --> Giữ nguyên\n",
        "    |\n",
        "    | Output: Motion Map [ B, T, C, H, W ] (Bản đồ nhiệt chuyển động)\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 4: TRỘN THÔNG TIN (FUSION)\n",
        "Code: fused = self.conv_fuse(cat([base, motion]))\n",
        "--------------------------------------------------------------------------------\n",
        "[ Flatten B & T ] --> Coi mỗi frame như một bức ảnh độc lập\n",
        "    |  Base:   [ B*T, C, H, W ]\n",
        "    |  Motion: [ B*T, C, H, W ]\n",
        "    |\n",
        "[ Concatenate ] --> Ghép chồng lên nhau\n",
        "    |  Result: [ B*T, 2C, H, W ]\n",
        "    |\n",
        "[ Conv2d (1x1) ] --> \"Nhào nặn\" thông tin hình dáng và chuyển động\n",
        "    |  Input: 2C channels --> Output: C channels\n",
        "    |  Result: Fused Feature [ B*T, C, H, W ]\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 5: CỘNG DƯ (RESIDUAL ADD)\n",
        "Code: out = base + alpha * fused\n",
        "--------------------------------------------------------------------------------\n",
        "[ Scale Alpha ] --> Alpha học được (ví dụ 0.5)\n",
        "    |               Quyết định xem nên thêm bao nhiêu \"gia vị\" chuyển động\n",
        "    |\n",
        "[ Addition ] --> Đặc trưng gốc + (Alpha * Đặc trưng chuyển động)\n",
        "    |\n",
        "    v\n",
        "[ Clamp ] --> Giới hạn giá trị [-1, 1] để ổn định mạng\n",
        "    |\n",
        "    v\n",
        "[ OUTPUT FINAL ] --> [ B, T, C, H, W ]\n",
        "================================================================================\n",
        "```"
      ],
      "metadata": {
        "id": "YJ5Cqebw1ZTD"
      },
      "id": "YJ5Cqebw1ZTD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ef42a32",
      "metadata": {
        "id": "9ef42a32"
      },
      "outputs": [],
      "source": [
        "class SMIFModule(nn.Module):\n",
        "    # SMIFModule (Spatial-Motion Interaction Fusion)\n",
        "    # Bắt các chuyển động ngắn hạn (Short-term motion) bằng cách So Sánh sự khác biệt giữa Các Frame Liền Kề.\n",
        "    # Capture short-term motion by comparing adjacent frames.\n",
        "    def __init__(self, channels: int, window_size: int = 5, alpha: float = 0.5, threshold: float = 0.05):\n",
        "        \"\"\"\n",
        "        Input: (B, T, C, H, W) - Video feature maps - batch, T for temporal frame_num, channel, hei, wit\n",
        "        Output: (B, T, C, H, W) - Video features đã được cường hóa thông tin chuyển động\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # sliding windows\n",
        "        assert window_size % 2 == 1, \"window_size must be odd\"\n",
        "        self.channels = channels\n",
        "        self.window_size = window_size\n",
        "        self.half = window_size // 2\n",
        "        self.threshold = threshold\n",
        "\n",
        "        # tạo tham số riêng Alpha (học đc giống weight)\n",
        "        # quyết định xem nên tin vào thông tin gốc\n",
        "        # hay thông tin chuyển động nhiều hơn.\n",
        "        self.alpha = nn.Parameter(torch.tensor(alpha))\n",
        "\n",
        "        # Conv 1x1 trộn thông tin (Fusion)\n",
        "        # Input channels * 2 vì ta nối (cat) đặc trưng gốc và bản đồ chuyển động lại\n",
        "        self.conv_fuse = nn.Conv2d(channels * 2, channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, video: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C, H, W = video.shape\n",
        "\n",
        "        # 1. Khởi tạo bản đồ tích lũy chuyển động để append thông tin vào sau\n",
        "        motion_accum = torch.zeros_like(video) #? CHÚ Ý GIÁ NÀO ĐC LƯU VÀO ĐÂY\n",
        "\n",
        "        #? Đơn giản là trừ pixel_future - pixel_current, pixel_current - pixel_past\n",
        "        # 2. Vòng lặp tính sai biệt (Difference) trong cửa sổ thời gian\n",
        "        # Duyệt từ offset = 1 đến radius (ví dụ window=5 thì offset chạy 1, 2)\n",
        "        for offset in range(1, self.half + 1):\n",
        "            # a. Dịch chuyển thời gian để lấy frame quá khứ và tương lai\n",
        "            # shifts=offset: Đẩy frame t-k đến vị trí t\n",
        "            prev_frames = torch.roll(video, shifts=offset, dims=1)\n",
        "            # shifts=-offset: Đẩy frame t+k đến vị trí t\n",
        "            next_frames = torch.roll(video, shifts=-offset, dims=1)\n",
        "\n",
        "            # b. Xử lý biên (Boundary Handling)\n",
        "            # Vì torch.roll xoay vòng (frame cuối về đầu), ta cần sửa lại.\n",
        "            # Frame đầu tiên không có quá khứ -> Gán bằng chính nó (độ lệch = 0)\n",
        "            prev_frames[:, :offset] = video[:, :offset]\n",
        "            # Frame cuối cùng không có tương lai -> Gán bằng chính nó\n",
        "            next_frames[:, -offset:] = video[:, -offset:]\n",
        "\n",
        "            # c. Tính sai biệt (Difference Calculation)\n",
        "            # Chuyển động = Sự thay đổi pixel giữa các frame\n",
        "            diff_f = next_frames - video  # Khác biệt với tương lai\n",
        "            diff_b = video - prev_frames  # Khác biệt với quá khứ\n",
        "\n",
        "            # d. Tích lũy độ lớn chuyển động (Absolute Difference)\n",
        "            motion_accum = motion_accum + diff_f.abs() + diff_b.abs()\n",
        "\n",
        "\n",
        "        # 3. Chuẩn hóa và lọc nhiễu (Filter nghĩ là Nếu > threshold -> apply Điều Kiện)\n",
        "        # Chia trung bình để giá trị không quá lớn\n",
        "        motion_map = motion_accum / max(self.half, 1)\n",
        "\n",
        "        # Tạo mask: Chỉ giữ lại những nơi có chuyển động mạnh hơn ngưỡng (threshold)\n",
        "        # Giúp loại bỏ nhiễu background hoặc rung lắc camera nhỏ\n",
        "        mask = (motion_map > self.threshold).float()\n",
        "        motion_map = motion_map * mask\n",
        "\n",
        "        # 4. Fusion (Trộn thông tin)\n",
        "        # Gộp batch và time lại để xử lý Conv2d song song: (B*T, C, H, W)\n",
        "        base = video.reshape(B * T, C, H, W)\n",
        "        motion_flat = motion_map.reshape(B * T, C, H, W)\n",
        "\n",
        "        # Nối đặc trưng gốc và motion map theo chiều channels -> (B*T, 2C, H, W)\n",
        "        fused = torch.cat([base, motion_flat], dim=1)\n",
        "\n",
        "        # Giảm chiều từ 2C về C, trộn thông tin không gian và chuyển động\n",
        "        fused = self.conv_fuse(fused)\n",
        "\n",
        "        # 5. Residual Connection & Gating\n",
        "        # Công thức: Out = Original + Alpha * Motion_Info\n",
        "        # Alpha dùng hàm tanh để giới hạn trọng số trong khoảng (-1, 1)\n",
        "        out = base + self.alpha.tanh() * fused\n",
        "\n",
        "        # Clamp để tránh giá trị bùng nổ (Numerical Stability)\n",
        "        out = out.clamp(min=-1.0, max=1.0)\n",
        "\n",
        "        # Trả về kích thước video ban đầu (B, T, C, H, W)\n",
        "        return out.view(B, T, C, H, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```js\n",
        "Hãy hình dung bài toán phân loại hành động \"Sút bóng\":\n",
        "+ Video dài 16 frames.\n",
        "+ Frame 1-5: Cầu thủ chạy đà (Chuyển động trung bình).\n",
        "+ Frame 6-8: Chân chạm bóng (Chuyển động rất mạnh, thay đổi đột ngột).\n",
        "+ Frame 9-16: Bóng bay đi, cầu thủ đứng lại (Chuyển động ít dần).\n",
        "\n",
        "Vai trò của LMI:\n",
        "\n",
        "1. Nó tính toán sự thay đổi giữa các frame và nhận ra Frame 6-8 có sự biến thiên lớn nhất (thông qua bước diff và mean).\n",
        "\n",
        "2. Nó gán attn cao (gần 1) cho các frame 6-8 và attn thấp cho các frame còn lại.\n",
        "\n",
        "3. Nhờ đó, mô hình Transformer sau đó sẽ \"tập trung\" sự chú ý vào khoảnh khắc sút bóng để đưa ra dự đoán chính xác, thay vì bị phân tâm bởi đoạn chạy đà.\n",
        "\n",
        "================================================================================\n",
        "              MINH HỌA WORKFLOW - LMI MODULE (Long-term Motion)\n",
        "================================================================================\n",
        "\n",
        "KÝ HIỆU:\n",
        "  B, T: Batch, Time (Frames)\n",
        "  N: Spatial Tokens (Số lượng Patch)\n",
        "  C: Channels (Gốc), Cr: Reduced Channels\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 1: GIẢM CHIỀU (REDUCTION)\n",
        "Code: reduced = self.reduce(x)\n",
        "--------------------------------------------------------------------------------\n",
        "[ INPUT ] Tensor X: [ B, T, N, C ]\n",
        "    |\n",
        "    v\n",
        "[ Linear Layer ] --> Nén C xuống Cr (để tính toán nhẹ hơn)\n",
        "    |\n",
        "    |  Reduced:    [ B, T, N, Cr ]\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 2: TÍNH SAI BIỆT THỜI GIAN (DIFFERENCE)\n",
        "Code: diff_f = ..., diff_b = ...\n",
        "--------------------------------------------------------------------------------\n",
        "[ Tính Diff ] --> So sánh Frame t với t+1 và t-1\n",
        "    |\n",
        "    |  Forward Diff:  |Frame(t+1) - Frame(t)|\n",
        "    |  Backward Diff: |Frame(t)   - Frame(t-1)|\n",
        "    |\n",
        "    |  Kết quả: Hai tensor [ B, T, N, Cr ]\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 3: TỔNG HỢP KHÔNG GIAN (SPATIAL AGGREGATION)\n",
        "Code: motion = (...).mean(dim=2)\n",
        "--------------------------------------------------------------------------------\n",
        "[ Sum Abs ] --> Cộng 2 hướng sai biệt: |F| + |B|\n",
        "    |\n",
        "[ Mean Pooling (dim N) ] --> QUAN TRỌNG!\n",
        "    |  Ta không quan tâm *vị trí nào* trong ảnh thay đổi.\n",
        "    |  Ta chỉ quan tâm *Frame nào* thay đổi nhiều nhất.\n",
        "    |  Làm phẳng chiều N.\n",
        "    |\n",
        "    |  Motion Vector: [ B, T, Cr ] (Mất chiều N)\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 4: TẠO TRỌNG SỐ (ATTENTION GENERATION)\n",
        "Code: attn = sigmoid(mlp(motion))\n",
        "--------------------------------------------------------------------------------\n",
        "[ MLP Block ] --> LayerNorm -> Linear -> GELU -> Linear\n",
        "    |  Tinh chỉnh đặc trưng chuyển động.\n",
        "    |\n",
        "[ Sigmoid ] --> Chuyển thành xác suất [0, 1]\n",
        "    |  Giá trị cao: Frame có nhiều chuyển động (quan trọng).\n",
        "    |  Giá trị thấp: Frame tĩnh (background/nhiễu).\n",
        "    |\n",
        "    |  Output: [ B, T, Cr ]\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 5: MỞ RỘNG & ÁP DỤNG (EXPAND & RE-WEIGHT)\n",
        "Code: enhanced = x * attn\n",
        "--------------------------------------------------------------------------------\n",
        "[ Expand Channel ] --> [ B, T, Cr ] -> [ B, T, C ] (về chiều sâu gốc)\n",
        "    |\n",
        "[ Unsqueeze & Expand Spatial ] --> [ B, T, 1, C ] -> [ B, T, N, C ]\n",
        "    |  Copy trọng số thời gian cho tất cả các patch không gian.\n",
        "    |\n",
        "[ Element-wise Mul ] --> Nhân Input gốc X với Attention Map\n",
        "    |  Frame quan trọng được giữ lại/tăng cường.\n",
        "    |  Frame vô nghĩa bị làm mờ đi.\n",
        "    |\n",
        "    |  Enhanced: [ B, T, N, C ]\n",
        "    v\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "BƯỚC 6: RESIDUAL ADDITION\n",
        "Code: return x + delta * enhanced\n",
        "--------------------------------------------------------------------------------\n",
        "[ Scale Delta ] --> Tham số học được.\n",
        "    |\n",
        "[ Add ] --> Input gốc + Thông tin đã cường hóa\n",
        "    |\n",
        "    |  OUTPUT FINAL: [ B, T, N, C ]\n",
        "    v\n",
        "================================================================================\n",
        "```"
      ],
      "metadata": {
        "id": "36UbPhxr5Ets"
      },
      "id": "36UbPhxr5Ets"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LMIModule(nn.Module):\n",
        "    def __init__(self, dim: int, reduction: int = 4, delta: float = 0.1):\n",
        "        \"\"\"\n",
        "        Input: (B, T, N, C) - Batch, Time, Tokens (Spatial), Channels\n",
        "        Output: (B, T, N, C) - Đặc trưng đã được tinh chỉnh dựa trên độ quan trọng của chuyển động\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Giảm chiều dữ liệu để tính toán motion nhẹ nhàng hơn\n",
        "        # Ví dụ: dim=768, reduction=4 -> reduced_dim=192\n",
        "        reduced_dim = max(1, dim // reduction)\n",
        "\n",
        "        self.reduce = nn.Linear(dim, reduced_dim)\n",
        "        self.expand = nn.Linear(reduced_dim, dim)\n",
        "\n",
        "        # MLP để xử lý thông tin chuyển động sau khi đã tổng hợp\n",
        "        self.temporal_mlp = nn.Sequential(\n",
        "            nn.LayerNorm(reduced_dim),\n",
        "            nn.Linear(reduced_dim, reduced_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(reduced_dim, reduced_dim),\n",
        "        )\n",
        "\n",
        "        # Delta: Tham số học được, kiểm soát mức độ ảnh hưởng của module này lên luồng chính\n",
        "        # Tương tự như alpha bên SMIFModule\n",
        "        self.delta = nn.Parameter(torch.tensor(delta))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, N, C = x.shape\n",
        "\n",
        "        # 1. Giảm chiều (Dimensionality Reduction)\n",
        "        # (B, T, N, C) -> (B, T, N, C/r)\n",
        "        reduced = self.reduce(x)\n",
        "\n",
        "        # 2. Tính toán sai biệt giữa các frame (Temporal Difference)\n",
        "        if T > 1:\n",
        "            # a. Sai biệt với tương lai (Forward Difference): Frame(t+1) - Frame(t)\n",
        "            # Lấy từ index 1 đến hết trừ đi từ index 0 đến áp chót\n",
        "            diff_f = reduced[:, 1:] - reduced[:, :-1]\n",
        "            # Padding: Frame cuối cùng không có tương lai, lặp lại giá trị sai biệt cuối\n",
        "            diff_f = torch.cat([diff_f, diff_f[:, -1:]], dim=1)\n",
        "\n",
        "            # b. Sai biệt với quá khứ (Backward Difference): Frame(t) - Frame(t-1)\n",
        "            diff_b = reduced[:, :-1] - reduced[:, 1:] # Về bản chất là sai biệt ngược\n",
        "            # Padding: Frame đầu tiên không có quá khứ, lặp lại giá trị sai biệt đầu\n",
        "            diff_b = torch.cat([diff_b[:, :1], diff_b], dim=1)\n",
        "        else:\n",
        "            # Nếu video chỉ có 1 frame thì không có chuyển động\n",
        "            diff_f = torch.zeros_like(reduced)\n",
        "            diff_b = torch.zeros_like(reduced)\n",
        "\n",
        "        # 3. Tổng hợp chuyển động (Aggregation)\n",
        "        # Cộng độ lớn sai biệt 2 chiều -> (B, T, N, C/r)\n",
        "        # .mean(dim=2): Trung bình cộng trên toàn bộ các Patch (N)\n",
        "        # Ý nghĩa: Tính xem TỔNG THỂ frame này thay đổi bao nhiêu so với frame kia.\n",
        "        # Shape: (B, T, N, C/r) -> (B, T, C/r)\n",
        "        motion = (diff_f.abs() + diff_b.abs()).mean(dim=2)\n",
        "\n",
        "        # 4. Tinh chỉnh đặc trưng chuyển động qua MLP\n",
        "        motion = self.temporal_mlp(motion)\n",
        "\n",
        "        # 5. Tạo Attention Map (Temporal Attention)\n",
        "        # Sigmoid đưa về (0, 1) để làm cổng (gate)\n",
        "        # unsqueeze(2): Thêm chiều N quay lại -> (B, T, 1, C/r)\n",
        "        attn = torch.sigmoid(motion).unsqueeze(2)\n",
        "\n",
        "        # 6. Mở rộng về kích thước gốc\n",
        "        # (B, T, 1, C/r) -> (B, T, 1, C) qua Linear expand\n",
        "        attn = self.expand(attn)\n",
        "        # (B, T, 1, C) -> (B, T, N, C) qua hàm expand (broadcast)\n",
        "        # Lúc này, tất cả các patch (N) trong cùng 1 frame (T) sẽ chịu chung 1 trọng số attention\n",
        "        attn = attn.expand(-1, -1, N, -1)\n",
        "\n",
        "        # 7. Áp dụng Attention (Enhancement)\n",
        "        # Những frame có chuyển động lớn (quan trọng) sẽ được khuếch đại\n",
        "        enhanced = x * attn\n",
        "\n",
        "        # 8. Residual Connection\n",
        "        # Cộng gộp vào luồng chính với trọng số delta\n",
        "        return x + self.delta.tanh() * enhanced"
      ],
      "metadata": {
        "id": "r2vPX0my40_H"
      },
      "id": "r2vPX0my40_H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```js\n",
        "================================================================================\n",
        "              MINH HỌA WORKFLOW - LSViT BLOCK\n",
        "================================================================================\n",
        "Input: Tensor [ B*T, N, C ] (Đang ở dạng \"Batch các ảnh tĩnh\")\n",
        "\n",
        "[ SPATIAL MODELING ]\n",
        "    |\n",
        "    +---> [ LayerNorm 1 ]\n",
        "    |\n",
        "    +---> [ Multi-Head Self-Attention ] --> Token nhìn thấy nhau trong CÙNG 1 frame\n",
        "    |\n",
        "    +---> [ Residual + DropPath ]\n",
        "    |\n",
        "    +---> [ LayerNorm 2 ]\n",
        "    |\n",
        "    +---> [ MLP ] --> Xử lý đặc trưng từng token\n",
        "    |\n",
        "    +---> [ Residual + DropPath ]\n",
        "    |\n",
        "    |  Kết quả: Đặc trưng không gian đã được tinh chỉnh\n",
        "    v\n",
        "\n",
        "[ RESHAPE FOR TEMPORAL ]\n",
        "    |  [ B*T, N, C ] ---> [ B, T, N, C ]\n",
        "    v\n",
        "\n",
        "[ TEMPORAL MODELING (LMI) ]\n",
        "    |\n",
        "    +---> [ LMI Module ] --> Frame t nhìn thấy Frame t-1, t+1...\n",
        "    |     (Cường hóa các frame có chuyển động quan trọng)\n",
        "    |\n",
        "    |  Kết quả: Đặc trưng đã có ngữ cảnh thời gian\n",
        "    v\n",
        "\n",
        "[ RESHAPE BACK ]\n",
        "    |  [ B, T, N, C ] ---> [ B*T, N, C ]\n",
        "    v\n",
        "Output: Tensor [ B*T, N, C ] (Sẵn sàng cho Block tiếp theo)\n",
        "================================================================================\n",
        "```"
      ],
      "metadata": {
        "id": "87HxZFPQ5hDb"
      },
      "id": "87HxZFPQ5hDb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06649681",
      "metadata": {
        "id": "06649681"
      },
      "outputs": [],
      "source": [
        "class LSViTBlock(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int, mlp_ratio: float, drop_rate: float, attn_drop: float, drop_path: float):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(dim, num_heads, True, attn_drop, drop_rate)\n",
        "        self.drop_path1 = DropPath(drop_path)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = Mlp(dim, mlp_ratio, drop_rate)\n",
        "        self.drop_path2 = DropPath(drop_path)\n",
        "        self.lmim = LMIModule(dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, B: int, T: int) -> torch.Tensor:\n",
        "        # Drop Path\n",
        "        x = x + self.drop_path1(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n",
        "        BT, Np1, C = x.shape\n",
        "        assert BT == B * T\n",
        "        x = x.view(B, T, Np1, C)\n",
        "        x = self.lmim(x)\n",
        "        x = x.view(B * T, Np1, C)\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qjkaCNdN7BZR"
      },
      "id": "qjkaCNdN7BZR"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LSViTBackbone(nn.Module):\n",
        "    # lắp ráp các mảnh ghép: PatchEmbed -> Positional Embedding -> Stack of Blocks.\n",
        "    def __init__(self, config: ViTConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # 1. Chuyển ảnh thành tokens\n",
        "        self.patch_embed = PatchEmbed(img_size=config.img_size, patch_size=config.patch_size,\n",
        "                                      in_chans=config.in_chans, embed_dim=config.embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # 2. CLS Token & Positional Embedding\n",
        "        # CLS Token: Vector đại diện cho toàn bộ bức ảnh/frame\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.embed_dim))\n",
        "        # Pos Embed: Giúp model biết vị trí của các patch (vì Transformer không biết trật tự)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, config.embed_dim))\n",
        "        self.pos_drop = nn.Dropout(config.drop_rate)\n",
        "\n",
        "        # 3. Drop Path Rate (Tăng dần theo độ sâu của mạng)\n",
        "        # Các block sâu hơn sẽ có xác suất bị drop cao hơn\n",
        "        dpr = torch.linspace(0, config.drop_path_rate, steps=config.depth).tolist()\n",
        "\n",
        "        # 4. Stack of LSViTBlocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            LSViTBlock(\n",
        "                dim=config.embed_dim,\n",
        "                num_heads=config.num_heads,\n",
        "                mlp_ratio=config.mlp_ratio,\n",
        "                drop_rate=config.drop_rate,\n",
        "                attn_drop=config.attn_drop_rate,\n",
        "                drop_path=dpr[i],\n",
        "            )\n",
        "            for i in range(config.depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(config.embed_dim)\n",
        "\n",
        "        # Khởi tạo trọng số (Weight Initialization)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "    def _interpolate_pos_encoding(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Hàm phụ trợ: Resize positional embedding nếu kích thước ảnh đầu vào thay đổi\"\"\"\n",
        "        B, N, C = x.shape\n",
        "        num_patches = N - 1 # Trừ đi CLS token\n",
        "\n",
        "        # Nếu số patch khớp với lúc train -> Dùng luôn, không cần chỉnh\n",
        "        if num_patches == self.patch_embed.num_patches:\n",
        "            return self.pos_embed\n",
        "\n",
        "        # Nếu khác (ví dụ finetune ảnh to hơn): Phải nội suy (interpolate)\n",
        "        cls_pos = self.pos_embed[:, :1]\n",
        "        patch_pos = self.pos_embed[:, 1:]\n",
        "        dim = patch_pos.shape[-1]\n",
        "\n",
        "        # Tính kích thước grid cũ và mới (Ví dụ 14x14 -> 16x16)\n",
        "        gs_old = int(math.sqrt(patch_pos.shape[1]))\n",
        "        gs_new = int(math.sqrt(num_patches))\n",
        "\n",
        "        # Reshape về dạng ảnh 2D để interpolate\n",
        "        patch_pos = patch_pos.reshape(1, gs_old, gs_old, dim).permute(0, 3, 1, 2)\n",
        "        patch_pos = F.interpolate(patch_pos, size=(gs_new, gs_new), mode=\"bicubic\", align_corners=False)\n",
        "\n",
        "        # Flatten lại về dạng sequence\n",
        "        patch_pos = patch_pos.permute(0, 2, 3, 1).reshape(1, gs_new * gs_new, dim)\n",
        "\n",
        "        return torch.cat([cls_pos, patch_pos], dim=1)\n",
        "\n",
        "    def forward(self, video: torch.Tensor) -> torch.Tensor:\n",
        "        # Input: (B, T, C, H, W) - Video 5D\n",
        "        B, T, C, H, W = video.shape\n",
        "\n",
        "        # 1. Gộp Batch và Time: Coi video là tập hợp các ảnh tĩnh lớn\n",
        "        # (B, T, C, H, W) -> (B*T, C, H, W)\n",
        "        x = video.reshape(B * T, C, H, W)\n",
        "\n",
        "        # 2. Patch Embedding: (B*T, N, C)\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # 3. Thêm CLS Token\n",
        "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1) # Copy CLS cho mọi mẫu\n",
        "        x = torch.cat((cls_tokens, x), dim=1) # (B*T, N+1, C)\n",
        "\n",
        "        # 4. Cộng Positional Embedding\n",
        "        pos_embed = self._interpolate_pos_encoding(x)\n",
        "        x = x + pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # 5. Đi qua các Transformer Blocks\n",
        "        # Truyền thêm B, T để các block biết cách reshape cho LMI\n",
        "        for block in self.blocks:\n",
        "            x = block(x, B, T)\n",
        "\n",
        "        # 6. Norm cuối cùng\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # 7. Khôi phục chiều thời gian\n",
        "        # Output: (B, T, N+1, C)\n",
        "        x = x.view(B, T, x.shape[1], x.shape[2])\n",
        "        return x"
      ],
      "metadata": {
        "id": "vjeDHPm67BMk"
      },
      "id": "vjeDHPm67BMk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```js\n",
        "================================================================================\n",
        "          HIGH-LEVEL WORKFLOW: LSViT FOR ACTION CLASSIFICATION\n",
        "================================================================================\n",
        "\n",
        "KÝ HIỆU:\n",
        "  B, T    : Batch Size, Time (Frames)\n",
        "  C, H, W : Channels, Height, Width\n",
        "  N       : Số lượng Patch Tokens (Spatial)\n",
        "  D       : Embedding Dimension (Ví dụ: 768)\n",
        "\n",
        "################################################################################\n",
        "#  PHẦN I: PIXEL-LEVEL PROCESSING (SMIF MODULE)                                #\n",
        "#  Mục tiêu: Cường hóa thông tin chuyển động ngắn hạn ngay trên điểm ảnh       #\n",
        "################################################################################\n",
        "\n",
        "[ INPUT VIDEO ]\n",
        "Tensor: [ B, T, C, H, W ]\n",
        "       |\n",
        "       v\n",
        "+-------------------------------------------------------+\n",
        "|  SMIF MODULE (Spatial-Motion Interaction Fusion)      |\n",
        "|                                                       |\n",
        "|  1. [Motion Calculation]                              |\n",
        "|     Tính sai biệt pixel: Frame(t) vs Frame(t±k)       |\n",
        "|                                                       |\n",
        "|  2. [Motion Map Generation]                           |\n",
        "|     Tạo bản đồ nhiệt (Heatmap) các vùng chuyển động   |\n",
        "|                                                       |\n",
        "|  3. [Fusion]                                          |\n",
        "|     Trộn ảnh gốc + Bản đồ chuyển động (Conv2d 1x1)    |\n",
        "+-------------------------------------------------------+\n",
        "       |\n",
        "       |  Output: Video đã được làm rõ chuyển động\n",
        "       v\n",
        "Tensor: [ B, T, C, H, W ]\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#  PHẦN II: FEATURE-LEVEL PROCESSING (LSViT BACKBONE)                          #\n",
        "#  Mục tiêu: Hiểu ngữ cảnh không gian (ảnh) và thời gian (video)               #\n",
        "################################################################################\n",
        "\n",
        "       | (Reshape: Gộp Batch và Time)\n",
        "       v\n",
        "Tensor: [ B*T, C, H, W ]\n",
        "       |\n",
        "       +---> [ Patch Partition & Embedding ] --> Cắt ảnh thành mảnh nhỏ\n",
        "       |     (Conv2d: kernel=patch_size, stride=patch_size)\n",
        "       |\n",
        "       +---> [ Add CLS Token & Positional Embed ]\n",
        "       |\n",
        "       v\n",
        "Tensor: [ B*T, N+1, D ] (Dạng chuỗi Token)\n",
        "\n",
        "       |\n",
        "       |  === BẮT ĐẦU VÒNG LẶP CÁC LSViT BLOCKS (x Depth) ===\n",
        "       |\n",
        "+------v-------------------------------------------------------+\n",
        "|  LSViT BLOCK                                                 |\n",
        "|                                                              |\n",
        "|  A. SPATIAL ATTENTION (Xử lý từng Frame độc lập)             |\n",
        "|     Input: [ B*T, N, D ]                                     |\n",
        "|     Action: Các token trong cùng 1 ảnh nhìn thấy nhau        |\n",
        "|     Result: Hiểu \"Cái gì\" đang ở trong ảnh                   |\n",
        "|                                                              |\n",
        "|     (Reshape tách B, T để xử lý thời gian)                   |\n",
        "|     [ B*T, N, D ] ---> [ B, T, N, D ]                        |\n",
        "|                                                              |\n",
        "|  B. TEMPORAL LMI MODULE (Xử lý Video)                        |\n",
        "|     Input: [ B, T, N, D ]                                    |\n",
        "|     Action: Frame(t) nhìn thấy Frame(t-1), Frame(t+1)        |\n",
        "|             Tăng trọng số cho frame có biến động lớn         |\n",
        "|     Result: Hiểu \"Hành động\" diễn ra thế nào                 |\n",
        "|                                                              |\n",
        "|     (Reshape gộp lại cho Block tiếp theo)                    |\n",
        "|     [ B, T, N, D ] ---> [ B*T, N, D ]                        |\n",
        "+--------------------------------------------------------------+\n",
        "       |\n",
        "       |  === KẾT THÚC VÒNG LẶP ===\n",
        "       v\n",
        "Tensor: [ B*T, N+1, D ] (Đặc trưng sâu sắc)\n",
        "       |\n",
        "       +---> [ Layer Norm ]\n",
        "       |\n",
        "       +---> [ Reshape Back ] --> Tách lại chiều thời gian T\n",
        "       |\n",
        "       v\n",
        "Tensor: [ B, T, N+1, D ]\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#  PHẦN III: CLASSIFICATION HEAD                                               #\n",
        "#  Mục tiêu: Tổng hợp thông tin và đưa ra dự đoán                              #\n",
        "################################################################################\n",
        "\n",
        "       |\n",
        "       +---> [ CLS Token Extraction ]\n",
        "       |     Chỉ lấy token đầu tiên (đại diện cho cả frame)\n",
        "       |     Bỏ qua N token không gian.\n",
        "       v\n",
        "Tensor: [ B, T, D ]\n",
        "\n",
        "       |\n",
        "       +---> [ Temporal Average Pooling ]\n",
        "       |     Tính trung bình cộng dọc theo trục T.\n",
        "       |     Gộp T frames thành 1 vector duy nhất.\n",
        "       v\n",
        "Tensor: [ B, D ] (Vector đặc trưng của Video)\n",
        "\n",
        "       |\n",
        "       +---> [ Linear Head ] (Fully Connected)\n",
        "       |     Chiếu từ chiều D sang số lượng Class (ví dụ 51)\n",
        "       v\n",
        "[ FINAL LOGITS ]\n",
        "Tensor: [ B, Num_Classes ]\n",
        "================================================================================\n",
        "```"
      ],
      "metadata": {
        "id": "TypJ3Png50AP"
      },
      "id": "TypJ3Png50AP"
    },
    {
      "cell_type": "code",
      "source": [
        "class LSViTForAction(nn.Module):\n",
        "    def __init__(self, config: ViTConfig, num_classes: int = 51, smif_window: int = 5):\n",
        "        super().__init__()\n",
        "        # 1. Module trích xuất chuyển động ngắn hạn (ngay từ pixel)\n",
        "        self.smif = SMIFModule(config.in_chans, window_size=smif_window)\n",
        "\n",
        "        # 2. Backbone chính (LS-ViT)\n",
        "        self.backbone = LSViTBackbone(config)\n",
        "\n",
        "        # 3. Head phân loại (Linear layer đơn giản)\n",
        "        # Input: Embed_dim -> Output: Num_classes (51 hành động)\n",
        "        self.head = nn.Linear(config.embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, video: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Input: (B, T, C, H, W)\n",
        "        Output: (B, num_classes) - Logits chưa qua Softmax\n",
        "        \"\"\"\n",
        "        # 1. Tiền xử lý chuyển động (SMIF)\n",
        "        # Video gốc được trộn thêm thông tin motion map\n",
        "        x = self.smif(video)\n",
        "\n",
        "        # 2. Feature Extraction (Backbone)\n",
        "        # Output: (B, T, N+1, C) - Đặc trưng của từng frame, từng patch\n",
        "        feats = self.backbone(x)\n",
        "\n",
        "        # 3. Lấy CLS Token\n",
        "        # Chỉ quan tâm token đầu tiên (index 0) của mỗi frame vì nó đại diện cho cả frame\n",
        "        # (B, T, N+1, C) -> (B, T, C)\n",
        "        cls_tokens = feats[:, :, 0]\n",
        "\n",
        "        # 4. Temporal Pooling (Trung bình theo thời gian)\n",
        "        # Gộp thông tin của T frames thành 1 vector duy nhất cho cả video\n",
        "        # (B, T, C) -> (B, C)\n",
        "        pooled = cls_tokens.mean(dim=1)\n",
        "\n",
        "        # 5. Phân loại\n",
        "        # (B, C) -> (B, Num_Classes)\n",
        "        logits = self.head(pooled)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "tooPkMr35z18"
      },
      "id": "tooPkMr35z18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc1b5415",
      "metadata": {
        "id": "bc1b5415"
      },
      "outputs": [],
      "source": [
        "class VideoTransform:\n",
        "    def __init__(self, image_size: int, is_train: bool = True):\n",
        "        self.image_size = image_size\n",
        "        self.is_train = is_train\n",
        "        self.mean = [0.5, 0.5, 0.5]\n",
        "        self.std = [0.5, 0.5, 0.5]\n",
        "\n",
        "    def __call__(self, frames: torch.Tensor) -> torch.Tensor:\n",
        "        # frames: [T, C, H, W]\n",
        "\n",
        "        if self.is_train:\n",
        "            # Random resized crop (scale 0.8-1.0)\n",
        "            h, w = frames.shape[-2:]\n",
        "            scale = random.uniform(0.8, 1.0)\n",
        "            new_h, new_w = int(h * scale), int(w * scale)\n",
        "            frames = TF.resize(frames, [new_h, new_w], interpolation=InterpolationMode.BILINEAR)\n",
        "\n",
        "            # Random crop to target size (for the entire video)\n",
        "            # áp dụng random crop cho toàn bộ các frame trong 1 video để đảm bảo Nhất quán về ko gian.\n",
        "            i = random.randint(0, max(0, new_h - self.image_size))\n",
        "            j = random.randint(0, max(0, new_w - self.image_size))\n",
        "            frames = TF.crop(frames, i, j, min(self.image_size, new_h), min(self.image_size, new_w))\n",
        "            frames = TF.resize(frames, [self.image_size, self.image_size], interpolation=InterpolationMode.BILINEAR)\n",
        "\n",
        "            # Horizontal flip\n",
        "            if random.random() < 0.5:\n",
        "                frames = TF.hflip(frames)\n",
        "\n",
        "            # Color jitter (brightness, contrast, saturation) - nhẹ\n",
        "            # augument ảnh ở nhiều điều kiện ánh sáng khác nhau\n",
        "            if random.random() < 0.3:\n",
        "                brightness_factor = random.uniform(0.9, 1.1)\n",
        "                frames = TF.adjust_brightness(frames, brightness_factor)\n",
        "\n",
        "            if random.random() < 0.3:\n",
        "                contrast_factor = random.uniform(0.9, 1.1)\n",
        "                frames = TF.adjust_contrast(frames, contrast_factor)\n",
        "\n",
        "            if random.random() < 0.3:\n",
        "                saturation_factor = random.uniform(0.9, 1.1)\n",
        "                frames = TF.adjust_saturation(frames, saturation_factor)\n",
        "        else:\n",
        "            # Val/test: center crop\n",
        "            frames = TF.resize(frames, [self.image_size, self.image_size], interpolation=InterpolationMode.BILINEAR)\n",
        "\n",
        "        # Normalize\n",
        "        normalized = [TF.normalize(frame, self.mean, self.std) for frame in frames]\n",
        "        return torch.stack(normalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8591522b",
      "metadata": {
        "id": "8591522b"
      },
      "outputs": [],
      "source": [
        "class HMDB51Dataset(Dataset):\n",
        "    \"\"\"Load HMDB51 frame folders with grouped train/val split.\"\"\"\n",
        "    def __init__(self, root: str, split: str, num_frames: int, frame_stride: int, image_size: int = 224, transform: Optional[VideoTransform] = None, val_ratio: float = 0.1, seed: int = 42):\n",
        "        super().__init__()\n",
        "        self.root = Path(root)\n",
        "        if not self.root.is_dir():\n",
        "            raise FileNotFoundError(f\"Data root not found: {self.root}\")\n",
        "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
        "        if not self.classes:\n",
        "            raise RuntimeError(f\"No class folders in {self.root}\")\n",
        "        self.class_to_idx = {name: idx for idx, name in enumerate(self.classes)}\n",
        "        grouped_samples: Dict[Tuple[str, str], List[Tuple[List[Path], int]]] = {}\n",
        "        for cls in self.classes:\n",
        "            cls_dir = self.root / cls\n",
        "            for video_dir in sorted([d for d in cls_dir.iterdir() if d.is_dir()]):\n",
        "                frame_paths = sorted([p for p in video_dir.iterdir() if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}])\n",
        "                if not frame_paths:\n",
        "                    continue\n",
        "                group_key = (cls, self._base_video_name(video_dir.name))\n",
        "                grouped_samples.setdefault(group_key, []).append((frame_paths, self.class_to_idx[cls]))\n",
        "        if not grouped_samples:\n",
        "            raise RuntimeError(f\"No frame folders found inside {self.root}\")\n",
        "        group_values = list(grouped_samples.values())\n",
        "        rng = np.random.RandomState(seed)\n",
        "        group_indices = np.arange(len(group_values))\n",
        "        rng.shuffle(group_indices)\n",
        "        split_point = int(len(group_indices) * (1 - val_ratio))\n",
        "        if split == \"train\":\n",
        "            selected_groups = group_indices[:split_point]\n",
        "        elif split in (\"val\", \"test\"):\n",
        "            selected_groups = group_indices[split_point:]\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown split: {split}\")\n",
        "        samples: List[Tuple[List[Path], int]] = []\n",
        "        for idx in selected_groups:\n",
        "            samples.extend(group_values[int(idx)])\n",
        "        if not samples:\n",
        "            raise RuntimeError(\"Selected split has no samples; adjust val_ratio or data folders.\")\n",
        "        self.samples = samples\n",
        "        self.split = split\n",
        "        self.num_frames = num_frames\n",
        "        self.frame_stride = max(1, frame_stride)\n",
        "        self.transform = transform or VideoTransform(image_size, is_train=(split == \"train\"))\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "    def _select_indices(self, total: int) -> torch.Tensor:\n",
        "        if total <= 0:\n",
        "            raise ValueError(\"Video folder has no frames\")\n",
        "        if total == 1:\n",
        "            return torch.zeros(self.num_frames, dtype=torch.long)\n",
        "        steps = max(self.num_frames * self.frame_stride, self.num_frames)\n",
        "        grid = torch.linspace(0, total - 1, steps=steps)\n",
        "        idxs = grid[:: self.frame_stride].long()\n",
        "        if idxs.numel() < self.num_frames:\n",
        "            pad = idxs.new_full((self.num_frames - idxs.numel(),), idxs[-1].item())\n",
        "            idxs = torch.cat([idxs, pad], dim=0)\n",
        "        return idxs[: self.num_frames]\n",
        "    @staticmethod\n",
        "    def _base_video_name(name: str) -> str:\n",
        "        match = re.match(r\"(.+)_\\d+$\", name)\n",
        "        return match.group(1) if match else name\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
        "        frame_paths, label = self.samples[idx]\n",
        "        total = len(frame_paths)\n",
        "        idxs = self._select_indices(total)\n",
        "        frames = []\n",
        "        for i in idxs:\n",
        "            path = frame_paths[int(i.item())]\n",
        "            with Image.open(path) as img:\n",
        "                img = img.convert(\"RGB\")\n",
        "                frames.append(self.to_tensor(img))\n",
        "        video = torch.stack(frames)\n",
        "        video = self.transform(video)\n",
        "        return video, label\n",
        "\n",
        "\n",
        "def collate_fn(batch: List[Tuple[torch.Tensor, int]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    videos = torch.stack([item[0] for item in batch])\n",
        "    labels = torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
        "    return videos, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fe2fcda",
      "metadata": {
        "id": "5fe2fcda"
      },
      "outputs": [],
      "source": [
        "def load_vit_checkpoint(backbone: LSViTBackbone, pretrained_name: str, weights_dir: str):\n",
        "    if timm is None:\n",
        "        raise ImportError(\"timm is required for auto-downloading pretrained weights\")\n",
        "    weights_path = Path(weights_dir)\n",
        "    weights_path.mkdir(parents=True, exist_ok=True)\n",
        "    auto_path = weights_path / f\"{pretrained_name}_timm.pth\"\n",
        "\n",
        "    if auto_path.is_file():\n",
        "        state = torch.load(auto_path, map_location=\"cpu\")\n",
        "    else:\n",
        "        print(f\"Downloading {pretrained_name} weights via timm...\")\n",
        "        pretrained_model = timm.create_model(pretrained_name, pretrained=True)\n",
        "        state = pretrained_model.state_dict()\n",
        "        torch.save(state, auto_path)\n",
        "\n",
        "    filtered_state = {}\n",
        "    for k, v in state.items():\n",
        "        if k.startswith(\"head\"):\n",
        "            continue\n",
        "        key = k\n",
        "        for prefix in (\"module.\", \"backbone.\"):\n",
        "            if key.startswith(prefix):\n",
        "                key = key[len(prefix):]\n",
        "        filtered_state[key] = v\n",
        "\n",
        "    missing, unexpected = backbone.load_state_dict(filtered_state, strict=False)\n",
        "    print(f\"Loaded ViT weights from {auto_path}\")\n",
        "    if missing:\n",
        "        print(\"Missing keys:\", missing)\n",
        "    if unexpected:\n",
        "        print(\"Unexpected keys:\", unexpected)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scaler, device, grad_accum_steps=1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    device_type = \"cuda\" if device.type == \"cuda\" else \"cpu\"\n",
        "    grad_accum_steps = max(1, grad_accum_steps)\n",
        "    num_batches = len(loader)\n",
        "    optimizer.zero_grad()\n",
        "    progress = tqdm(loader, desc=\"Train\", leave=False)\n",
        "    for batch_idx, (videos, labels) in enumerate(progress):\n",
        "        videos = videos.to(device)\n",
        "        labels = labels.to(device)\n",
        "        with amp.autocast(device_type=device_type, enabled=(device.type == \"cuda\")):\n",
        "            logits = model(videos)\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        loss_value = loss.item()\n",
        "        loss = loss / grad_accum_steps\n",
        "        scaler.scale(loss).backward()\n",
        "        should_step = ((batch_idx + 1) % grad_accum_steps == 0) or (batch_idx + 1 == num_batches)\n",
        "        if should_step:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "        batch_size = videos.size(0)\n",
        "        total_loss += loss_value * batch_size\n",
        "        progress.set_postfix(\n",
        "            loss=f\"{loss_value:.4f}\",\n",
        "            acc=f\"{correct / max(total, 1):.4f}\"\n",
        "        )\n",
        "    avg_loss = total_loss / max(total, 1)\n",
        "    avg_acc = correct / max(total, 1)\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.0\n",
        "    progress = tqdm(loader, desc=\"Val\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for videos, labels in progress:\n",
        "            videos = videos.to(device)\n",
        "            labels = labels.to(device)\n",
        "            logits = model(videos)\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            progress.set_postfix(\n",
        "                acc=f\"{correct / max(total, 1):.4f}\",\n",
        "                loss=f\"{total_loss / max(total, 1):.4f}\"\n",
        "            )\n",
        "    avg_loss = total_loss / max(total, 1)\n",
        "    return correct / max(total, 1), avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67999a5e",
      "metadata": {
        "id": "67999a5e"
      },
      "outputs": [],
      "source": [
        "DATA_ROOT = './hmdb51_data'\n",
        "VAL_RATIO = 0.1\n",
        "SEED = 42\n",
        "NUM_FRAMES = 16\n",
        "FRAME_STRIDE = 2\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "train_dataset = HMDB51Dataset(\n",
        "    root=DATA_ROOT,\n",
        "    split='train',\n",
        "    num_frames=NUM_FRAMES,\n",
        "    frame_stride=FRAME_STRIDE,\n",
        "    image_size=IMG_SIZE,\n",
        "    val_ratio=VAL_RATIO,\n",
        "    seed=SEED,\n",
        ")\n",
        "val_dataset = HMDB51Dataset(\n",
        "    root=DATA_ROOT,\n",
        "    split='val',\n",
        "    num_frames=NUM_FRAMES,\n",
        "    frame_stride=FRAME_STRIDE,\n",
        "    image_size=IMG_SIZE,\n",
        "    val_ratio=VAL_RATIO,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=1,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "MEAN = [0.5, 0.5, 0.5]\n",
        "STD = [0.5, 0.5, 0.5]\n",
        "\n",
        "def denormalize(frames):\n",
        "    frames = frames.clone()\n",
        "    for c in range(frames.shape[1]):\n",
        "        frames[:, c] = frames[:, c] * STD[c] + MEAN[c]\n",
        "    return frames.clamp(0, 1)\n",
        "\n",
        "print(f'Train clips: {len(train_dataset)} | Val clips: {len(val_dataset)}')\n",
        "print(f'Class count: {len(train_dataset.classes)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2602a3ab",
      "metadata": {
        "id": "2602a3ab"
      },
      "outputs": [],
      "source": [
        "sample_idx = 0\n",
        "sample_frames, sample_label = train_dataset[sample_idx]\n",
        "vis_frames = denormalize(sample_frames).cpu()\n",
        "class_name = train_dataset.classes[sample_label]\n",
        "frames_to_show = min(vis_frames.shape[0], 12)\n",
        "cols = 4\n",
        "rows = math.ceil(frames_to_show / cols)\n",
        "plt.figure(figsize=(12, 3 * rows))\n",
        "for i in range(frames_to_show):\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    frame = vis_frames[i].permute(1, 2, 0).numpy()\n",
        "    plt.imshow(frame)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Frame {i + 1}')\n",
        "plt.suptitle(f'Sample clip from class: {class_name}', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "454de73e",
      "metadata": {
        "id": "454de73e"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "GRAD_ACCUM_STEPS = 16\n",
        "BASE_LR = 5e-5\n",
        "HEAD_LR = 2.5e-4\n",
        "WEIGHT_DECAY = 0.05\n",
        "PRETRAINED_NAME = 'vit_base_patch16_224'\n",
        "\n",
        "weights_dir = './weights'\n",
        "Path(weights_dir).mkdir(parents=True, exist_ok=True)\n",
        "best_ckpt = Path(weights_dir) / 'lsvit_hmdb51_best.pt'\n",
        "last_ckpt = Path(weights_dir) / 'lsvit_hmdb51_last.pt'\n",
        "\n",
        "config = ViTConfig(image_size=IMG_SIZE)\n",
        "model = LSViTForAction(config).to(DEVICE)\n",
        "load_vit_checkpoint(model.backbone, PRETRAINED_NAME, weights_dir)\n",
        "\n",
        "backbone_params = []\n",
        "head_params = []\n",
        "for name, param in model.named_parameters():\n",
        "    if not param.requires_grad:\n",
        "        continue\n",
        "    if name.startswith(\"backbone\"):\n",
        "        backbone_params.append(param)\n",
        "    else:\n",
        "        head_params.append(param)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [\n",
        "        {\"params\": backbone_params, \"lr\": BASE_LR},\n",
        "        {\"params\": head_params, \"lr\": HEAD_LR},\n",
        "    ],\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "best_acc = 0.0\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        scaler,\n",
        "        DEVICE,\n",
        "        grad_accum_steps=GRAD_ACCUM_STEPS,\n",
        "    )\n",
        "    val_acc, val_loss = evaluate(model, val_loader, DEVICE)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    torch.save({'model': model.state_dict(), 'acc': val_acc}, last_ckpt)\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save({'model': model.state_dict(), 'acc': best_acc}, best_ckpt)\n",
        "\n",
        "    print(\n",
        "        f'Epoch {epoch + 1}/{EPOCHS} | train_loss={train_loss:.4f} | train_acc={train_acc:.4f} | '\n",
        "        f'val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | best={best_acc:.4f}'\n",
        "    )\n",
        "\n",
        "trained_model = model\n",
        "training_history = history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d844d38f",
      "metadata": {
        "id": "d844d38f"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(training_history['train_loss']) + 1)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
        "\n",
        "axes[0].plot(epochs, training_history['train_loss'], label='Train Loss')\n",
        "axes[0].plot(epochs, training_history['val_loss'], label='Val Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Loss Curves')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(epochs, training_history['train_acc'], label='Train Accuracy', color='blue')\n",
        "axes[1].plot(epochs, training_history['val_acc'], label='Val Accuracy', color='green')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Accuracy Curves')\n",
        "axes[1].set_ylim(0, 1)\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dbe29f4",
      "metadata": {
        "id": "0dbe29f4"
      },
      "outputs": [],
      "source": [
        "trained_model.eval()\n",
        "if best_ckpt.is_file():\n",
        "    state = torch.load(best_ckpt, map_location=DEVICE)\n",
        "    trained_model.load_state_dict(state['model'])\n",
        "\n",
        "with torch.no_grad():\n",
        "    sample_idx = random.randrange(len(val_dataset))\n",
        "    sample_video, sample_label = val_dataset[sample_idx]\n",
        "    logits = trained_model(sample_video.unsqueeze(0).to(DEVICE))\n",
        "    probs = torch.softmax(logits, dim=1).cpu().squeeze(0)\n",
        "\n",
        "pred_idx = int(probs.argmax())\n",
        "pred_class = val_dataset.classes[pred_idx]\n",
        "true_class = val_dataset.classes[sample_label]\n",
        "confidence = float(probs[pred_idx])\n",
        "\n",
        "vis_frames = denormalize(sample_video).cpu()\n",
        "frames_to_show = min(vis_frames.shape[0], 12)\n",
        "cols = 4\n",
        "rows = math.ceil(frames_to_show / cols)\n",
        "plt.figure(figsize=(12, 3 * rows))\n",
        "for i in range(frames_to_show):\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    frame = vis_frames[i].permute(1, 2, 0).numpy()\n",
        "    plt.imshow(frame)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Frame {i + 1}')\n",
        "plt.suptitle(\n",
        "    f'Predicted: {pred_class} ({confidence:.2%}) | Ground Truth: {true_class}', fontsize=14\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}