{"cells":[{"cell_type":"markdown","metadata":{"id":"BdE3415GA72R"},"source":["## **0. Tải bộ dữ liệu**\n","**Lưu ý:** Nếu bạn không thể sử dụng lệnh gdown để tải bộ dữ liệu vì bị giới hạn số lượt tải, hãy tải bộ dữ liệu thử công và upload lên google drive của mình. Sau đó, sử dụng lệnh dưới đây để copy file dữ liệu vào colab:\n","```python\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","!cp /path/to/dataset/on/your/drive .\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"urzyheIkVQFv","outputId":"45677693-9349-4028-9922-90d5a82afd04"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -qq faiss-cpu\n","!pip install -qq transformers\n","!pip install -qq pandas\n","!pip install -qq numpy\n","!pip install -qq scikit-learn\n","!pip install -qq tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHfGc0o2fZMu"},"outputs":[],"source":["# https://drive.google.com/file/d/1N7rk-kfnDFIGMeX0ROVTjKh71gcgx-7R/view?usp=sharing\n","!gdown --id 1N7rk-kfnDFIGMeX0ROVTjKh71gcgx-7R"]},{"cell_type":"markdown","metadata":{"id":"dVvC6obsBYG4"},"source":["## **1. Import các thư viện cần thiết**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tU6nV0YHhWWm"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import faiss\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"vX0MeDrnBcY1"},"source":["## **2. Đọc bộ dữ liệu**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KoeUNKOclJK9"},"outputs":[],"source":["DATASET_PATH = '/content/2cls_spam_text_cls.csv'\n","df = pd.read_csv(DATASET_PATH)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQblTvxTnIPj"},"outputs":[],"source":["messages = df['Message'].values.tolist()\n","labels = df['Category'].values.tolist()"]},{"cell_type":"markdown","metadata":{"id":"WadqE8dtBf9Q"},"source":["## **3. Chuẩn bị embedding model và dữ liệu**"]},{"cell_type":"markdown","metadata":{"id":"d2FJWXIABiwa"},"source":["### **3.1. Load embedding model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xzBrlBKslOz5"},"outputs":[],"source":["# Load embedding model\n","MODEL_NAME = 'intfloat/multilingual-e5-base'\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","model.eval()\n","\n","print(f'Using device: {device}')\n","print(f'Model loaded: {MODEL_NAME}')\n","\n","def average_pool(last_hidden_states, attention_mask):\n","    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n","    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]"]},{"cell_type":"markdown","metadata":{"id":"X7aTeSvzBogY"},"source":["### **3.2. Tạo sentence embeddings**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_m5jsZBlh5P"},"outputs":[],"source":["def get_embeddings(texts, model, tokenizer, device, batch_size=32):\n","    \"\"\"Generate embeddings for a list of texts\"\"\"\n","    embeddings = []\n","\n","    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n","        batch_texts = texts[i:i+batch_size]\n","\n","        # Add passage prefix for better retrieval performance\n","        batch_texts_with_prefix = [f\"passage: {text}\" for text in batch_texts]\n","\n","        # Tokenize\n","        batch_dict = tokenizer(batch_texts_with_prefix,\n","                              max_length=512,\n","                              padding=True,\n","                              truncation=True,\n","                              return_tensors='pt')\n","\n","        # Move to device\n","        batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n","\n","        # Generate embeddings\n","        with torch.no_grad():\n","            outputs = model(**batch_dict)\n","            batch_embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n","            # Normalize embeddings\n","            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n","            embeddings.append(batch_embeddings.cpu().numpy())\n","\n","    return np.vstack(embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5KV1Kpwmh7y"},"outputs":[],"source":["# Prepare labels\n","le = LabelEncoder()\n","y = le.fit_transform(labels)\n","print(f'Classes: {le.classes_}')\n","\n","# Generate embeddings for all messages\n","print(f\"Generating embeddings for {len(messages)} messages...\")\n","X_embeddings = get_embeddings(messages, model, tokenizer, device)\n","print(f\"Embeddings shape: {X_embeddings.shape}\")\n","\n","# Create metadata for each document\n","metadata = []\n","for i, (message, label) in enumerate(zip(messages, labels)):\n","    metadata.append({\n","        'index': i,\n","        'message': message,\n","        'label': label,\n","        'label_encoded': y[i]\n","    })\n","\n","print(f\"Created metadata for {len(metadata)} documents\")"]},{"cell_type":"markdown","metadata":{"id":"ti67ymhoVQFz"},"source":["### **3.3. Tạo FAISS index và chia dữ liệu**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tEy3OrleoARz"},"outputs":[],"source":["# Split data into train and test (90% train, 10% test)\n","TEST_SIZE = 0.1\n","SEED = 42\n","\n","train_indices, test_indices = train_test_split(\n","    range(len(messages)),\n","    test_size=TEST_SIZE,\n","    stratify=y,\n","    random_state=SEED\n",")\n","\n","# Split embeddings and metadata\n","X_train_emb = X_embeddings[train_indices]\n","X_test_emb = X_embeddings[test_indices]\n","y_train = y[train_indices]\n","y_test = y[test_indices]\n","\n","train_metadata = [metadata[i] for i in train_indices]\n","test_metadata = [metadata[i] for i in test_indices]\n","\n","print(f\"Train size: {len(X_train_emb)}\")\n","print(f\"Test size: {len(X_test_emb)}\")\n","print(f\"Train label distribution: {np.bincount(y_train)}\")\n","print(f\"Test label distribution: {np.bincount(y_test)}\")\n","\n","# Create FAISS index\n","embedding_dim = X_train_emb.shape[1]\n","index = faiss.IndexFlatIP(embedding_dim)  # Inner product for cosine similarity\n","index.add(X_train_emb.astype('float32'))\n","\n","print(f\"FAISS index created with {index.ntotal} vectors\")"]},{"cell_type":"markdown","metadata":{"id":"AvzSFGPyB83P"},"source":["## **4. Implement classification với embedding similarity**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKm5qxIWhJmT"},"outputs":[],"source":["def classify_with_knn(query_text, model, tokenizer, device, index, train_metadata, k=1):\n","    \"\"\"Classify text using k-nearest neighbors with embeddings\"\"\"\n","\n","    # Get query embedding\n","    query_with_prefix = f\"query: {query_text}\"\n","    batch_dict = tokenizer([query_with_prefix],\n","                          max_length=512,\n","                          padding=True,\n","                          truncation=True,\n","                          return_tensors='pt')\n","\n","    batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**batch_dict)\n","        query_embedding = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n","        query_embedding = F.normalize(query_embedding, p=2, dim=1)\n","        query_embedding = query_embedding.cpu().numpy().astype('float32')\n","\n","    # Search in FAISS index\n","    scores, indices = index.search(query_embedding, k)\n","\n","    # Get predictions from top-k neighbors\n","    predictions = []\n","    neighbor_info = []\n","\n","    for i in range(k):\n","        neighbor_idx = indices[0][i]\n","        neighbor_score = scores[0][i]\n","        neighbor_label = train_metadata[neighbor_idx]['label']\n","        neighbor_message = train_metadata[neighbor_idx]['message']\n","\n","        predictions.append(neighbor_label)\n","        neighbor_info.append({\n","            'score': float(neighbor_score),\n","            'label': neighbor_label,\n","            'message': neighbor_message[:100] + \"...\" if len(neighbor_message) > 100 else neighbor_message\n","        })\n","\n","    # Majority vote for final prediction\n","    unique_labels, counts = np.unique(predictions, return_counts=True)\n","    final_prediction = unique_labels[np.argmax(counts)]\n","\n","    return final_prediction, neighbor_info\n","\n","def evaluate_knn_accuracy(test_embeddings, test_labels, test_metadata, index, train_metadata, k_values=[1, 3, 5]):\n","    \"\"\"Evaluate accuracy for different k values using precomputed embeddings\"\"\"\n","    results = {}\n","    all_errors = {}\n","\n","    for k in k_values:\n","        correct = 0\n","        total = len(test_embeddings)\n","        errors = []\n","\n","        for i in tqdm(range(total), desc=f\"Evaluating k={k}\"):\n","            query_embedding = test_embeddings[i:i+1].astype('float32')\n","            true_label = test_metadata[i]['label']\n","            true_message = test_metadata[i]['message']\n","\n","            # Search in FAISS index\n","            scores, indices = index.search(query_embedding, k)\n","\n","            # Get predictions from top-k neighbors\n","            predictions = []\n","            neighbor_details = []\n","            for j in range(k):\n","                neighbor_idx = indices[0][j]\n","                neighbor_label = train_metadata[neighbor_idx]['label']\n","                neighbor_message = train_metadata[neighbor_idx]['message']\n","                neighbor_score = float(scores[0][j])\n","\n","                predictions.append(neighbor_label)\n","                neighbor_details.append({\n","                    'label': neighbor_label,\n","                    'message': neighbor_message,\n","                    'score': neighbor_score\n","                })\n","\n","            # Majority vote\n","            unique_labels, counts = np.unique(predictions, return_counts=True)\n","            predicted_label = unique_labels[np.argmax(counts)]\n","\n","            if predicted_label == true_label:\n","                correct += 1\n","            else:\n","                # Collect error information\n","                error_info = {\n","                    'index': i,\n","                    'original_index': test_metadata[i]['index'],\n","                    'message': true_message,\n","                    'true_label': true_label,\n","                    'predicted_label': predicted_label,\n","                    'neighbors': neighbor_details,\n","                    'label_distribution': {label: int(count) for label, count in zip(unique_labels, counts)}\n","                }\n","                errors.append(error_info)\n","\n","        accuracy = correct / total\n","        error_count = total - correct\n","\n","        results[k] = accuracy\n","        all_errors[k] = errors\n","\n","        print(f\"Accuracy with k={k}: {accuracy:.4f}\")\n","        print(f\"Number of errors with k={k}: {error_count}/{total} ({(error_count/total)*100:.2f}%)\")\n","\n","    return results, all_errors"]},{"cell_type":"markdown","metadata":{"id":"2FTBzozgBuIx"},"source":["## **5. Đánh giá accuracy trên test set**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"anE-mJw5qndx"},"outputs":[],"source":["%%time\n","# Evaluate accuracy for different k values\n","print(\"Evaluating accuracy on test set...\")\n","accuracy_results, error_results = evaluate_knn_accuracy(\n","    X_test_emb,\n","    y_test,\n","    test_metadata,\n","    index,\n","    train_metadata,\n","    k_values=[1, 3, 5]\n",")\n","\n","# Display results\n","print(\"\\n\" + \"=\"*50)\n","print(\"ACCURACY RESULTS\")\n","print(\"=\"*50)\n","for k, accuracy in accuracy_results.items():\n","    print(f\"Top-{k} accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n","print(\"=\"*50)\n","\n","# Save error analysis to JSON file\n","import json\n","from datetime import datetime\n","\n","error_analysis = {\n","    'timestamp': datetime.now().isoformat(),\n","    'model': MODEL_NAME,\n","    'test_size': len(X_test_emb),\n","    'accuracy_results': accuracy_results,\n","    'errors_by_k': {}\n","}\n","\n","for k, errors in error_results.items():\n","    error_analysis['errors_by_k'][f'k_{k}'] = {\n","        'total_errors': len(errors),\n","        'error_rate': len(errors) / len(X_test_emb),\n","        'errors': errors\n","    }\n","\n","# Save to JSON file\n","output_file = 'error_analysis.json'\n","with open(output_file, 'w', encoding='utf-8') as f:\n","    json.dump(error_analysis, f, ensure_ascii=False, indent=2)\n","\n","print(f\"\\n***Error analysis saved to: {output_file}***\")\n","print()\n","print(f\"***Summary:\")\n","for k, errors in error_results.items():\n","    print(f\"   k={k}: {len(errors)} errors out of {len(X_test_emb)} samples\")\n"]},{"cell_type":"markdown","metadata":{"id":"ZCoOf4BrBwYI"},"source":["## **6. Pipeline classification cho user input**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9qcQ_4Pqpes"},"outputs":[],"source":["def spam_classifier_pipeline(user_input, k=3):\n","    \"\"\"\n","    Complete pipeline for spam classification\n","\n","    Args:\n","        user_input (str): Text to classify\n","        k (int): Number of nearest neighbors to consider\n","\n","    Returns:\n","        dict: Classification results with details\n","    \"\"\"\n","\n","    print()\n","    print(f\"***Classifying: '{user_input}'\")\n","    print()\n","    print(f\"***Using top-{k} nearest neighbors\")\n","    print()\n","\n","    # Get prediction and neighbors\n","    prediction, neighbors = classify_with_knn(\n","        user_input, model, tokenizer, device, index, train_metadata, k=k\n","    )\n","\n","    # Display results\n","    print(f\"***Prediction: {prediction.upper()}\")\n","    print()\n","\n","    print(\"***Top neighbors:\")\n","    for i, neighbor in enumerate(neighbors, 1):\n","        print(f\"{i}. Label: {neighbor['label']} | Score: {neighbor['score']:.4f}\")\n","        print(f\"   Message: {neighbor['message']}\")\n","        print()\n","\n","    # Count label distribution\n","    labels = [n['label'] for n in neighbors]\n","    label_counts = {label: labels.count(label) for label in set(labels)}\n","\n","    return {\n","        'prediction': prediction,\n","        'neighbors': neighbors,\n","        'label_distribution': label_counts\n","    }"]},{"cell_type":"markdown","metadata":{"id":"X1GS4YWRByPB"},"source":["## **7. Test pipeline với các ví dụ**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-BVohFfpqmJ8"},"outputs":[],"source":["# Test với các ví dụ khác nhau\n","test_examples = [\n","    \"I am actually thinking a way of doing something useful\",\n","    \"FREE!! Click here to win $1000 NOW! Limited time offer!\",\n","    # \"Hey, can you pick me up at 5pm today?\",\n","    # \"URGENT: Your account will be suspended unless you verify your details NOW\",\n","    # \"Thanks for the meeting today, let's schedule the next one for next week\",\n","    # \"Congratulations! You've won a prize! Call this number to claim it\"\n","]\n","\n","print(\"Testing pipeline with different examples:\")\n","print()\n","\n","for i, example in enumerate(test_examples, 1):\n","    print(f\"\\n***Example {i}:\")\n","    result = spam_classifier_pipeline(example, k=3)\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaL0bg3io8a2"},"outputs":[],"source":["# Interactive testing - user có thể thay đổi text và k value\n","print(\"***Interactive Testing\")\n","print()\n","\n","# Người dùng có thể thay đổi các giá trị này để test với các ví dụ khác nhau\n","user_text = \"Win a free iPhone! Click here now!\"\n","k_value = 5\n","\n","print(f\"***Testing with k={k_value}\")\n","result = spam_classifier_pipeline(user_text, k=k_value)\n","\n","print(\"***To test with different inputs:\")\n","print(\"1. Change 'user_text' variable above\")\n","print(\"2. Change 'k_value' for different number of neighbors\")\n","print(\"3. Re-run this cell\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"}},"nbformat":4,"nbformat_minor":0}